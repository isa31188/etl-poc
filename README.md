Table of Contents
=================

* [TL;DR ](#tldr")
* [Solution architecture](#solution-architecture")
* [Configuring the project](#configure-project")
    * [Project requirements](#project-requirements")
    * [Deploying the resources](#deploy-resources")
    * [Running the ETL workflow](#run-workflow")
    * [Checking schemas](#check-schemas")
    * [Running queries on extracted data](#run-queries")
    * [Deleting resources](#delete-resources")
* [Notes on architectural decisions](#architectural-decisions")
* [Possible improvements](#improvements")
* [Some notes on costs](#costs")


<a name="tldr"></a>
# TL;DR 

This repo contains a simple data ingestion use case comprised of an external data source, ETL (extract-transform-load) pipeline, data catalog and querying platform. The solution was developed in AWS using the following services: 

* **Cloudformation**, for the management of AWS resources, 
* **IAM**, for access roles for computing instances, 
* **S3**: for storing data and artifact code, 
* **Glue**: for ETL pipelines (Python and Pyspark) and data catalog,
* **Athena**: for querying data. 

This solution was partially inspired by this repo: [aws-samples/aws-etl-orchestrator](https://github.com/aws-samples/aws-etl-orchestrator).


<a name="solution-architecture"></a>
# Solution architecture

A more detailed description of the resources used in this project, by AWS service:

* **Cloudformation stacks** for managing AWS resources. For ease of development, these were grouped into:
    + `basic-infra` stack: basic resources required for deploying the projects, namely, buckets for artifacts and data and Athena work group for querying data storing results in S3.
    + `glue-jobs` stack: set of etl jobs for extracting, cleaning and repartitioning data.
    + `glue-catalog` stack: Glue database and crawlers for automatic generation of table metadata.
    + `glue-workflow` stack: orchestration between etl jobs and data crawlers.

![Cloudformation Stacks](images/cloudformation_stacks.png)

* **S3 buckets** used for storing data, query results and code artifacts required by the etl jobs:
    + `artifacts bucket`: for storing python and pyspark source code,
    + `raw stage data bucket`: for storing the data directly downloaded from the source,
    + `silver stage data bucket`: for storing the data after standardizing processing,
    + `gold stage data bucket`: for storing the data after standardizing and re-structuring,
    + `athena queries bucket`: for storing the results of queries run in Athena.

![S3 Buckets](images/buckets.png)

* **IAM roles** used to define access profiles for ETL jobs and crawlers:
    + `GluePythonJobRole`: execution role for python-based etl jobs,
    + `GlueSparkJobRole`: execution role for pyspark-based etl jobs,
    + `CrawlerRole`: execution for for data crawlers.

![IAM Roles](images/iam_roles.png)

* **Glue jobs** for running extract-transform-load jobs:
    + `extract job`: downloading the data from the source and unzipping it, storing it in its original format
    + `cleaning job`: correcting data types, cleaning column names, droping duplicated records, storing data in parquet format
    + `repartition job`: repartitioning data by country and storing in parquet format

Job definitions:
![Glue Jobs](images/glue_jobs.png)

Artifact code used in the etl jobs:
![Artifacts](images/bucket_artifacts.png)

* **Glue Database** for storing the metadata of the tables generated by the ETL jobs. This metadata must be defined in order to make the data accessible in Athena for querying. Table metadata is automatically generated by the implemented crawlers, as described next.

![Catalog](images/glue_tables.png)

* **Glue Crawlers** for automatic generation of table metadata, required for making the extracted and transformed data available in Athena for querying:
    + `raw data crawler`: automatically creates a Glue Table definition for the data directly downloaded from the source (raw stage).
    + `silver stage crawler`: automatically creates a Glue Table definition for the data generated by the cleaning ETL job (silver stage).
    + `gold stage crawler`: automatically creates a Glue Table definition for the data generated by the repartitioning ETL job (gold stage).

Crawlers:
![Crawlers](images/glue_crawlers.png)

An example of metadata generated by the `gold stage crawler`:
![Table Properties](images/glue_table_properties.png)
![Table Schema](images/glue_table_schema.png)

* **Athena work group**: defines the setup for querying the data in this project, namely, what S3 bucket is used for storing query results. Can also be used for limiting data usage, for cost control.

![Athena Workgroup](images/athena_workgroup.png)

* **Glue Workflow** and **Triggers**: orchestrate between the Glue ETL jobs and the Glue Crawlers. In this project, triggers are based on user action (on-demand trigger) or events. The workflow logic is as follows (image below):
    + `on-demand workflow trigger`: starts the data extraction job,
    + `successful data extraction`: triggers the raw data crawler and the cleaning job,
    + `successful data cleaning`: triggers the silver data crawler and the repartition job,
    + `successful repartition`: triggers the gold data crawler.

![Glue Workflow](images/glue_workflow.png)


<a name="configure-project"></a>
# Configuring the project

<a name="project-requirements"></a>
## Project requirements

The data ingestion solution in this repo is based on a series of AWS resources. Folder `cloudformation/` contains the series of stacks defining those resources. These stacks can be built using the script [build.py](build.py) in the root of the repo (script adapted from [aws-samples/aws-etl-orchestrator](https://github.com/aws-samples/aws-etl-orchestrator)).

Running `build.py` for deploying these stack requires:

* python (version 3.9)
* AWS CLI configured with full access to the following:
    + Amazon S3
    + Amazon CloudWatch and CloudWatch Logs
    + AWS CloudFormation
    + Creating IAM Roles
* python libraries `pynt` and `boto3` (libraries used for facilitating the local management of cloudformation stacks)


<a name="deploy-resources"></a>
## Deploying the resources

Given how the resources for this project were organized into different stacks, in order to completely deploy them, the following commands must be executed in this specific order:

```
pynt createstack[basic-infra]
pynt deploygluescripts
pynt createstack[glue-jobs]
pynt createstack[glue-catalog]
pynt createstack[glue-workflow]
```

The stacks are defined by the `.yml` files under the `cloudformation/` folder. Each stack uses the parameters defined in the respective `.json` file, under the same folder.

<a name="run-workflow"></a>
## Running the ETL workflow

If all commands in the previous section run successfully, you can execute the full ETL workflow (Glue Jobs and Crawlers) directly from the "AWS Glue" console. On the left bar, click on "Workflows (orchestration)". Select the workflow named `etl_workflow` and click on "Run workflow". In alternative, run in a local terminal `aws glue start-workflow-run --name etl-workflow`.

<a name="check-schemas"></a>
## Checking schemas

Once the workflow has completely run, you can check the generated metadata also in the "AWS Glue" console under: `Data Catalog > Databases > Tables`.


<a name="run-queries"></a>
## Running queries on extracted data



<a name="delete-resources"></a>
## Deleting resources

In order to delete the cloudformation stacks, the following commands must be run in this specific order:

```
pynt deletestack[glue-workflow]
pynt deletestack[glue-catalog]
pynt deletestack[glue-jobs]
deleteS3bucket[etl-poc-artifacts]
deleteS3bucket[etl-poc-athena-queries]
deleteS3bucket[etl-poc-data-raw]
deleteS3bucket[etl-poc-data-silver]
deleteS3bucket[etl-poc-data-gold]
pynt deletestack[basic-infra]
```

Note that deleting the `basic-infra` stack requires that the objects in the respective buckets are delete beforehand.



<a name="architectural-decisions"></a>
# Notes on architectural decisions

The solution in this repo implements both the ETL jobs and metadata using the AWS Glue service, contrary to other architectures commonly found in the literature which use both Glue and Lambda functions. Some reasons for not using Lambda functions:

* Lambda functions and Glue jobs have very similar format (python scripts). However, Glue Job runtimes suport a broader set of libraries and provide seamless pyspark integration. The codebase is therefore simpler and thus easier to maintain.
* Eventual limitations of Glue jobs are easily overcome, e.g. triggering by specific services. 
* Longer execution time and higher cost of Glue Jobs is, at least for the current use case, negligible. Also, the project was developed under the assumption of one-time or sporadic execution of the ETL pipeline.

<a name="improvements"></a>
# Possible improvements

This project was developed with the goal of having a functional solution for an ingestion pipeline on a tight timeline. Several suboptimal architectural decisions were therefore taken to allow fast development. 

Possible improvements are:

* Automating project lifetime management with CI/CD processes
* Re-organizing cloudformation stacks
* Considering Terraform in alternative to Cloudformation for infrastructure management 
* Refining IAM role access policies
* Adding data quality checks
* Defining data access profiles (e.g. access only granted to gold stage)
* Setting data usage control limits in Athena
* Setting up alarms for job execution failure


<a name="costs"></a>
# Some notes on costs

Add something to this section ... 